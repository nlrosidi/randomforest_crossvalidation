{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning with Random Forest and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module dives into machine learning algorithms, specifically Random Forest, to predict events based on a set of attributes. This can be used for offer users specific recommendations based on their information or it can be used to assess the most important product features that leads to users performing specific actions. Whatever question you're trying to solve, machine learning techniques can be used to better understand complex interactions and attributes that are often difficult to tease out using standard statistical methods like t-tests. \n",
    "\n",
    "This module acts more as a framework for you to implement whatever machine learning algorithm you want to implement, as I'm concentrating more on the process and implementation rather than the specific statistical model itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import, cleaning, and preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a dummy dataset that lists people that have purchased health insurance. People are listed by their attributes (e.g., gender, age, industry they work in, household size, the number of children in their household, and the user's relationship to the person that purchased the health insurance. Lastly, we list whether or not the person has registered on the insurer's website (this is our target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosidin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import dataset as a pandas dataframe\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% of data science is data prep and cleaning. After importing our dataset, we want to make sure we understand how python is interpreting every variable and datapoint. We want to also dig around to see if there are any nulls or other values that might throw off the analysis. \n",
    "\n",
    "You should always perform the below three commands on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   registered gender  age                           industry  household_size  \\\n",
      "0           0      M   51  Health Care and Social Assistance             2.0   \n",
      "1           0      F   29  Health Care and Social Assistance             2.0   \n",
      "2           0      M   38  Health Care and Social Assistance             2.0   \n",
      "3           0      F   39  Health Care and Social Assistance             4.0   \n",
      "4           0      F   32  Health Care and Social Assistance             3.0   \n",
      "\n",
      "   dependents relationship  \n",
      "0           0       spouse  \n",
      "1           0       spouse  \n",
      "2           0         self  \n",
      "3           1       spouse  \n",
      "4           1         self  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      "registered        1000 non-null int64\n",
      "gender            1000 non-null object\n",
      "age               1000 non-null int64\n",
      "industry          1000 non-null object\n",
      "household_size    850 non-null float64\n",
      "dependents        1000 non-null int64\n",
      "relationship      1000 non-null object\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 54.8+ KB\n",
      "None\n",
      "        registered          age  household_size  dependents\n",
      "count  1000.000000  1000.000000      850.000000  1000.00000\n",
      "mean      0.379000    36.966000        3.041176     0.45500\n",
      "std       0.485381    13.541574        1.685369     0.49822\n",
      "min       0.000000    18.000000        1.000000     0.00000\n",
      "25%       0.000000    24.000000        2.000000     0.00000\n",
      "50%       0.000000    35.000000        3.000000     0.00000\n",
      "75%       1.000000    48.000000        4.000000     1.00000\n",
      "max       1.000000    79.000000        9.000000     1.00000\n"
     ]
    }
   ],
   "source": [
    "#describe dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with dirty data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like python was able to interpret the data types correctly. Wherever there is a number, python interpretes the datatype as float or int, and wherever there's a string, the datatype is an object. \n",
    "\n",
    "However, it looks like we have a few nulls in the household_size variable so we need to deal with this accordingly. I'm just going to drop the rows where household_size is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 850 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      "registered        850 non-null int64\n",
      "gender            850 non-null object\n",
      "age               850 non-null int64\n",
      "industry          850 non-null object\n",
      "household_size    850 non-null float64\n",
      "dependents        850 non-null int64\n",
      "relationship      850 non-null object\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 53.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# deal with nulls and nas\n",
    "df = df.dropna(how='any', subset=['household_size'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Processing and handling numerical data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some variables have a large variance and some small, our model will bias towards the large variances. \n",
    "For example if you change one variable from km to cm (increasing its variance), it may go from having little \n",
    "impact to dominating all the other variables in the model. \n",
    "\n",
    "If you want your model to be independent of such rescaling, standardizing the variables will do that. \n",
    "On the other hand, if the specific scale of your variables matters (in that you want your model to be in that scale), \n",
    "maybe you don't want to standardize.\n",
    "\n",
    "In this example, we're going to normalize by transforming the vector so that it has unit norm. When data are thought \n",
    "of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be \n",
    "normal, normalizing means transforming to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "df['age_normal'] = (df['age']- df['age'].mean())/(df['age'].std())\n",
    "df['hh_normal'] = (df['household_size']- df['household_size'].mean())/(df['household_size'].std())\n",
    "df['deps_normal'] = (df['dependents']- df['dependents'].mean())/(df['dependents'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dummifying the data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of our variables are categorical, we need to split out the different categories into variables. For example, gender can be male or female. When we dummify, we make male and female into variables of their own (let's name the variables gender_male and gender_female). A male will then have a 1 under gender_male and a 0 under gender_female, thus transforming a categorical variable into a numerical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dummify data\n",
    "dummy_gender = pd.get_dummies(df.gender, prefix='gender')\n",
    "dummy_relationship = pd.get_dummies(df.relationship, prefix='relationship')\n",
    "dummy_industry = pd.get_dummies(df.industry, prefix='industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've transformed our categorical variables into numerical ones, we can delete the categorical variables and join the dummy variables into the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop original columns that have been normalized or dummified\n",
    "df = df.drop(['gender','relationship','age','industry','household_size','dependents'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join dummy data columns to dataframe\n",
    "df = df.join(dummy_gender)\n",
    "df = df.join(dummy_relationship)\n",
    "df = df.join(dummy_industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 850 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      "registered                                                   850 non-null int64\n",
      "age_normal                                                   850 non-null float64\n",
      "hh_normal                                                    850 non-null float64\n",
      "deps_normal                                                  850 non-null float64\n",
      "gender_F                                                     850 non-null uint8\n",
      "gender_M                                                     850 non-null uint8\n",
      "relationship_self                                            850 non-null uint8\n",
      "relationship_spouse                                          850 non-null uint8\n",
      "industry_Educational Services                                850 non-null uint8\n",
      "industry_Finance and Insurance                               850 non-null uint8\n",
      "industry_Health Care and Social Assistance                   850 non-null uint8\n",
      "industry_Information                                         850 non-null uint8\n",
      "industry_Manufacturing                                       850 non-null uint8\n",
      "industry_Other Services (except Public Administration)       850 non-null uint8\n",
      "industry_Professional, Scientific, and Technical Services    850 non-null uint8\n",
      "industry_Public Administration                               850 non-null uint8\n",
      "industry_Retail Trade                                        850 non-null uint8\n",
      "industry_Transportation and Warehousing                      850 non-null uint8\n",
      "industry_Unknown                                             850 non-null uint8\n",
      "industry_Utilities                                           850 non-null uint8\n",
      "industry_Wholesale Trade                                     850 non-null uint8\n",
      "dtypes: float64(3), int64(1), uint8(17)\n",
      "memory usage: 87.3 KB\n"
     ]
    }
   ],
   "source": [
    "#let's take a look at our new dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using a Random Forest Classifier which constructs several decision trees to generate classifications (i.e., predictions) based on variables in the dataset. Briefly, a random forest is made up of many decisions trees where each tree gives a classification (i.e., votes). The forest then chooses the classification having the most votes.\n",
    "\n",
    "We'll split our variables into features and target. The target is the classification itself -- did the user register or not? The features are the user attributes such as their age, household size, number of dependents, industry, and gender. So the question is -- can we accurately predict whether or not a user will register based on their attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify the feature and target dataset\n",
    "features = df[1:].values\n",
    "target = df['registered'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split our dataset into a training and test set. Our model with train the algorithm using the training set and then test the model on the test set, giving us an accuracy score as an output. We also want to estimate how accurately our model makes predictions. So in order to do both we use KFold() which will give us as many indexes as we want for training and testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(features, target, classifier, k_fold) :\n",
    "    '''Calculates average accuracy of classification \n",
    "    algorithm using kfold crossvalidation'''\n",
    "    # derive a set of (random) training and testing indices\n",
    "    k_fold_indices = KFold(len(features), n_folds=k_fold,\n",
    "                           shuffle=True, random_state=0)\n",
    "    # for each training and testing slices run the classifier, and score the results\n",
    "    k_score_total = 0\n",
    "    for train_slice, test_slice in k_fold_indices :\n",
    "        model = classifier.fit(features[train_slice],\n",
    "                               target[train_slice])\n",
    "        k_score = model.score(features[test_slice],\n",
    "                              target[test_slice])\n",
    "        k_score_total += k_score\n",
    "    # return the average accuracy\n",
    "    return k_score_total/k_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to test the accuracy of our model as we change the number of decision trees in the forest. We want to see how the number of trees affect the accuracy of our results and hopefully find a maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 estimators for RF - 0.6419561434041071\n",
      "20 estimators for RF - 0.658489383919248\n",
      "30 estimators for RF - 0.6525931082492168\n",
      "40 estimators for RF - 0.6561225200139227\n",
      "50 estimators for RF - 0.6702610511660286\n"
     ]
    }
   ],
   "source": [
    "for n in range(10,51,10):\n",
    "    model = RandomForestClassifier(n)\n",
    "    score = cross_validate(features, target, model, 5)\n",
    "    print('{0} estimators for RF - {1}'.format(n, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our results show that we can achieve ~66% accuracy in predicting whether or not a user registers based on their attributes. That's not really very accurate but it's better than a coin flip."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
